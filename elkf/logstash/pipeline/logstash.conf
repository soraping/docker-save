input {
    beats {
        # 当前logstash开放端口，beats基于tcp
        port => 5044
        # filebeats 的IP地址
        host => "127.0.0.1"
    }

    tcp {
        mode => "server"
        # spring项目地址
        host => "127.0.0.1"
        port => "8088"
        codec => "plain"
    }

    kafka {
        bootstrap_servers => "127.0.0.1:9092"  #kafka的地址，替换为你自己的 
		client_id => "logstash"  
	    auto_offset_reset => "latest"  
	    consumer_threads => 5
	    topics => ["likeadminapi", "spingcloudlog"]  #获取哪些topic，在springboot项目的logback-spring.xml中指定
        type => "json"  #输出的结果也就是message中的信息以json的格式展示
        codec => json {
            charset => "UTF-8"
        }
    }
}

# 过滤
filter {
    if[app_name] == "derui" {
        grok {
            match => { "message" => "\s*\[\s*%{WORD:level}\s*\]\s*date\[%{TIMESTAMP_ISO8601:timestamp}:\]\s* host\[%{URIHOST:http_host}\]\s* ip\[%{IP:clientIp}\]\s* logId\[%{INT:logId}\]\s* uri\[(?<request>[\w,/]+)\]\s* time_used\[\d+\]\s* %{GREEDYDATA:msg}" }
        }
        mutate {
            rename => {"msg" => "msgInfo"}
            remove_field => ["port"]
        }
    }

    date {
        match => ["timestamp", "yyyy-MM-dd HH:mm:ss","ISO8601"]
        target => "@timestamp"
        timezone => "Asia/Shanghai"
    }
}

# 输出到es
output {
    stdout {
        codec => rubydebug
    }

    if [@metadata][kafka][topic] == "spingcloudlog" {
        elasticsearch {
            hosts => "http://es:9200"
            user => "elastic"
            password => "123456"
            index => "spingcloudlog-%{+YYYY.MM.dd}"   #这里将会是创建的索引名，后续 kibana将会用不同索引区别
            timeout => 300
        }
    }

    if[app_name] == "derui" {
        elasticsearch {
            hosts => ["http://es:9200"]
            user => "elastic"
            password => "123456"
            # 索引名称，这里获取filebeat设置的参数
            index => "derui-%{+YYYY.MM.dd}"
            codec  => "json"
        }
    }





}